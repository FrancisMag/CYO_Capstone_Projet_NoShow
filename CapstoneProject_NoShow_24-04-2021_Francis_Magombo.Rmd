---
title: 'HarvardX PH125.9x Data Science Capstone Project: Noshow'
author: "Francis Magombo"
date: "20/03/2021"
output:
  pdf_document: 
    latex_engine: lualatex
    toc: yes
    number_sections: yes
  html_document: default
  word_document: 
    toc: yes
---

```{r global.options, include = TRUE, echo=FALSE, message = FALSE, warning=FALSE}
knitr::opts_chunk$set(
    cache       = TRUE,     # if TRUE knitr will cache the results to reuse in future knits
    fig.width   = 10,       # the width for plots created by code chunk
    fig.height  = 8,       # the height for plots created by code chunk
    fig.align   = 'center', # how to align graphics in the final doc. 'left', 'right', 'center'
    fig.path    = 'figs/',  # file path to the directory where knitr shall store the graphics files
    results     = 'asis',   # knitr will pass through results without reformatting them
    echo        = TRUE,     # in FALSE knitr will not display code in the code chunk above it's results
    message     = TRUE,     # if FALSE knitr will not display any messages generated by code
    strip.white = TRUE,     # if FALSE knitr will not remove white spaces at the beg or end of code chunk
    warning     = FALSE)    # if FALSE knitr will not display any warning messages in the final document
```

\pagebreak

```{r install packages, echo = FALSE, message=FALSE, warning=FALSE}

#install packages
if(!require(RCurl)) install.packages("RCurl", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(DescTools)) install.packages("DescTools", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(vioplot)) install.packages("vioplot", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("tidyr", repos = "http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(broom)) install.packages("(broom", repos = "http://cran.us.r-project.org")
if(!require(memisc)) install.packages("memisc", repos = "http://cran.us.r-project.org")
if(!require(pander)) install.packages("pander", repos = "http://cran.us.r-project.org")
```



```{r upload libraries, echo = FALSE, warning=FALSE, message=FALSE}
#Uploading the packages:

library(RCurl) # for downloading the dataset files
library(tidyverse)   # general
library(caret)       # model
library(DescTools)   # descriptive statistics
library(ggplot2)  # visualisation
library(dplyr) # data wrangling
library(tidyr) #reshaping
library (stringr) #managing strings
library(lubridate) #dates
library(broom) #for tidy()
library(kableExtra)
library(memisc) #for tables
library(pander)
```

Downloading the data files:

```{r dataset download, echo = TRUE, message=FALSE} 
nstmpfile <- tempfile()
download.file("https://raw.githubusercontent.com/FrancisMag/CYO_Capstone_Projet_NoShow/main/KaggleV2-May-2016.csv", method = "curl", nstmpfile)

data <- read.csv(nstmpfile, stringsAsFactors =TRUE, header=TRUE)
```

```{r remove tempfile, echo = FALSE, message=FALSE} 

unlink(nstmpfile)
```

# Introduction

We will use predictive machine learning algorithms to model and predict what factors are important in determining ifr the patient will show up or not.  Predictive machine learning algorithms have become more and more useful in providing insights and solutions or predictions towards existing or emerging problems and hence assisting with in planning and making decisions for the future.The methods used in machine learning will essentially depend on the type of data that is available and also the nature of the problem you are trying to solve. The different methods used include classification models, random forest, k-nearest neighbors among other machine learning algorithms that exist. In this projects we will use logistic regression, random forest, k-nearest neighbors as well as ensemble methods to try and find the most accurate prediction method for the factors influencing the patients to miss their appointments(noshows).


# Dataset and variables

Total number of observations and variables are tabulate as below:

```{r observations_variables, echo = TRUE}

dim(data) 
```

The"Noshow_KaggleV2-May-2016" data, which we will use in this project can be found on https://www.kaggle.com/wbadry/noshow-appointment-may-2016

This dataset collects information from 110,527 medical appointments in Brazil and is focused on the question of whether or not patients show up for their appointments. A number of characteristics about the patient are included in each row. Some of the factors under consideration are whether the age, gender, neighborhood or receiving a reminder message influence the noshows or whether it is other additional factors like times period between appointment date and scheduled date or whether it is the neighbourhood where the patients live that are key to follow through with the appointments.

The list of variables is as outlined below:

```{r variable_list, echo=FALSE}
names(data) %>% knitr::kable()
```

The variables used in this dataset are: PatientId, AppointmentID, Gender, ScheduledDay, AppointmentDay, Age, Neighbourhood, Scholarship, Hipertension, Diabetes, Alcoholism, Handcap, SMS_received, and No.show. We see that the the rest of the variables are in character class except for the Handcap, Hipertension, Alcoholism, Age, Scholarship,AppointmentID, PatientId and  SMS_received.Below we further explore the nature of this data before we plan for its analysis or any modelling that it can allow. 

We can see that out of 110,527 patients around 22,319 of them did not show up, which is about 20%.

```{r number of noshows, echo=FALSE}
data %>%
  group_by(No.show) %>%
  count() %>% knitr::kable()
```


We disaggregate the number of males and females in order to have a clear picture of the numbers of both groups.

```{r number of males and females, echo=FALSE}
count(data, Gender) %>% knitr::kable()
```

We also need to know the observations for the different age groups. This is made clear by the graph below.

```{r Age_frequency, echo = FALSE}
ggplot(data, aes(Age))+geom_freqpoly(binwidth = 1)  +
  xlab("Age") + 
  ylab("Frequency") +
  ggtitle("Observations for different ages")
```

The data is collected from eighty-one (81) neighbourhoods and contains fourteen (14) variables. The total number of observations is 110,527. There are 71,840 females and 38,687 males in the dataset. The age group is from -1 to 115 years and the mean is 37 years. It is unlikely that one would have an age of less than zero and that there would be seven patients with an age above 100 years. So the analysis will need to exclude those of less than zero age and those above 100 years old. The  population is distributed unevenly in the different neighborhoods with one neighborhood having a population of above 5,700 and the next close to 6,000 whilst some neighborhoods have close to zero participants.


# Goal of the project

The aim of this project is to make come up with a prediction model for the patients that are likely to miss their medical appointment using the variables in the available dataset. We will explore and model using the variables and the different modelling methods to discover which model gives the most accurate prediction for the noshows. 

# Methods and Analysis

Factors to be tested:We will explore the variables available and already described above to see which ones could be included in the models.We will start by considering the demographic factors.

## Data Cleaning

We first note that some of the variables are in character class format and will need to be converted to factor class format to facilitate easy calculations in the models to be used. The variables concerned include:Gender, Neighbourhood,  Scholarship,  Hipertension,  Diabetes, Alcoholism, SMS_received, and No.show. 

Change the data type of some columns:

```{r change_to_factor, echo= TRUE}
data <- mutate_at(data, 
                  vars('Gender', 'Neighbourhood',  'Scholarship',  
                       'Hipertension',  'Diabetes', 'Alcoholism', 
                       'SMS_received', 'No.show'), as.factor)
```

Next we convert the variable Handcap to binary form so that we only assess whether one is handicapped or not.The "Handcap" is described at data description as a binary variable but the values in the column range from 0 to 4 which should not be. Therefore, we will consider any value of "Handcap" bigger than 0 as 1 in this case. We note thet 2,241 (2%) of the patients were handicapped.

We will keep the Handcap variable into a binary form maintaining only two levels fr the factr varable.

```{r Handcap_binary_nature, echo=FALSE}
data$Handcap <- as.factor(ifelse(data$Handcap > 0 ,1,0))
summary(data$Handcap) %>% knitr::kable()
```

Instead to keeping the "No"or "Yes"in the No.show variable we will convert "Yes" to "1" and "No" to "0" to maintain a similar format being used in the other categorical variables and for easy calculation. 

```{r change No.show labels to 1 or 0, echo = FALSE}
library(tidyverse)
data$No.show <- as.factor(ifelse(data$No.show == "Yes", 1, 0))
summary(data$No.show) %>% knitr::kable()
```
Likewise we will do the same for the variable "Gender". We note that 71,840 (65.0%) of the patients were female.

```{r Change Gender labels to 1 or 0, echo = FALSE}
data$Gender <- as.factor(ifelse(data$Gender == "M", 1, 0))
summary(data$Gender) %>% knitr::kable()
```

We also will change the variable "Age" from the character class format to numeric class format. 

```{r Change Age to numeric class, echo = FALSE}

    data$Age <- as.numeric(data$Age)
```


We then check is there are any  missing values.It is evident that there are no missing values in the dataset.

Missing values:

```{r missing_values, echo=FALSE}
sapply(data,function(x)sum(is.na(x))) %>% knitr::kable()
```

We noted that the ages of the patients ranged from -1 to 115 years. To make more practical calculations we will need to keep the data that has the most appropriate age range which in this case we will consider to be above zero and up to 100 years. We now need to remove the one below zero years of age and above 100 years.


```{r Maintaining age 0-100, echo=FALSE}

data <-data[(data$Age > 0) & (data$Age <= 100),]
```

The updated patient age group distribution as per the selected lower limit (0) and upper limit (100 years) is shown in this graph.

```{r Age_distribution, echo=TRUE}
ggplot(data=data, aes(x=Age)) + geom_histogram(binwidth =2) +
  xlab("Age") + 
  ylab("Frequency") +
  ggtitle("Observations for different ages")
    ```


We see that the age range of the noshows is younger than that of those who actually turned up for their appointments.

Plot Age and No.show status:

```{r Age_Noshow, echo=FALSE}
ggplot(data,aes(y=Age,fill=No.show,x=factor(1))) +geom_boxplot() +
xlab("Noshow status")+ylab("Age range") + ggtitle("Observations for No.Show")
```

## Modeling approach

We select the variables:Age, Gender, Scholarship,  Hipertension,  Diabetes, Alcoholism, SMS_received, No.show to be used and remove AppointmentDay, ScheduledDay, and PatientId from the first model. 

```{r selecting the initial model variables, echo = TRUE} 

data <- data %>% 
  dplyr::select('Age', 'Gender', 'Scholarship',  'Hipertension',
                'Diabetes', 'Alcoholism', 'SMS_received', 'No.show')
```

The variable "Neighbourhood" was also removed later on because randomForest could not handle  a variable with more than 53 categories and so presented an error message.

We will take a small sample of the dataset for the analysis given the size of the dataset in comparison to the capacity of the laptop computer being used.
 
```{r sample a smaller subset of data, echo = TRUE, warning=FALSE}
    set.seed(1998, sample.kind = "Rounding")
    
    #Create index vector
    idx <- seq_len(nrow(data))
    
   #Sample from the index vector
    samp <- sample(idx, 10000)
    
    #create a dataframe of 10,000 random rows
    data_samp <- data[samp, ]
```
    
## Data Partitioning (Training and Test data sets)

We will now develop an algorithm using only the training set.The validation(test) subset will be 10 percent of the total data. The test set will be used to evaluate the result from the training set. The 10 percent was chosen arbitrarily as the proportion that is mostly used in other machine learning algorithms. Since we have categorical data we will be reporting on the  proportion of the No.shows that will be correctly predicted in the test set using the overall accuracy measure.The overall accuracy is simply defined as the overall proportion that is predicted correctly. Validation set will be 10% of the data.

```{r data_partition, echo=FALSE, warning=FALSE}
set.seed(2011, sample.kind = "Rounding")
test_index <- createDataPartition(y = data_samp$No.show, times = 1, p = 0.1, list = FALSE)
train_set <- data_samp[-test_index,]
test_set <- data_samp[test_index,]
```

## Modeling results

```{r load packages1, echo = FALSE}
library(caret)
library(tidyverse)
```

### Model 1: logistic regression

We start by training logistic regression model which we test afterwards.

Training the logistic regression model:

```{r, train_logistic regression model, echo=TRUE, warning=FALSE}
train_glm <- train(No.show ~., method = "glm", data = train_set)

pander(summary(train_glm))
```

Making a prediction for the logistic regression model:

```{r, predict_logistic regression model, echo=TRUE}
y_hat_glm <- predict(train_glm, test_set, type = "raw")
```

Evaluating the logistic regression model:

```{r logistic regression model evaluation, echo = TRUE}
confusionMatrix(y_hat_glm, test_set$No.show)$overall[["Accuracy"]]
```

We then evaluate the accuracy of the logistic regression model. Which we find to be 79.7%. We note that the variables Age, Scholarship1, SMS_received1 have significant contribution to the patients not showing up at for their appointments in this model. The Age is negatively correlated with not showing up (older patients are more prone to keep their appointments), having scholarship is positively correlated to the patient not showing up and receiving and SMS also is positively correlated to the noshow (the majority of those who received the sms still did not show up for their appointments). The accuracy of the prediction model is 79.7%. we will try to use the significant variables only for the next logistic regression model.

```{r logistic regression2_prediction with only the significant variables, echo = TRUE, warning=FALSE}
train_glm2 <- train(No.show ~ Age + Scholarship + SMS_received, method = "glm", data = train_set)

pander(summary(train_glm2))

y_hat_glm2 <- predict(train_glm2, test_set, type = "raw")

confusionMatrix(y_hat_glm2, test_set$No.show)$overall[["Accuracy"]]
```

The accuracy of the  model prediction does not change if we only include the variables that had been shown to have a significant contribution to the first logistic regression model (Age, Scholarship and SMS_received).It remains 79.7. Therefore we will go ahead and use all the variables initially selected for the first logistic regression model for the subsequent machine learning algorithms.In the next model we will use the kNN method to make the prediction.
 
### Model 2: kNN

```{r load packages2, echo = FALSE}
library(caret)
library(tidyverse)
``` 

Training the kNN model: 

```{r knn_train, echo = TRUE, warning=FALSE}
 train_knn <- train(No.show ~ ., method = "knn", data = train_set)

 pander(summary(train_knn))
```
 
Making the kNN model prediction:

```{r knn_predict, echo = TRUE} 
 y_hat_knn <- predict(train_knn, test_set, type = "raw")

``` 

Evaluating the accuracy of the model: 

```{r knn-accuracy, echo = TRUE}
 confusionMatrix(y_hat_knn, test_set$No.show)$overall[["Accuracy"]]
```

The accuracy in the kNN model is 79% which is below the one for the logistic regression model 79.7%. We also try applying the randomForest model using the same variables.
 
### Model 3 : randomForest

```{r load packages3, echo = FALSE, message = FALSE} 
library(randomForest)
library("e1071")
library(rpart)
```

The first randomForest method using ntree (500, 1000, 1500, 2000) and mtry 3:8 had an accuracy of 79.5%. The second method with ntree (500, 750,1000, 1500, 2000) and mtry 3:7 had an accuracy of 79.6%. However, we run only the the selected, the second, model due to the lengthy time it takes to complete each one of them. 
 
The selected randomForest model is the one below:

```{r randomForest train-model, echo = TRUE, warning=FALSE}
 rf_ranges <- list(ntree = c(500, 1000, 1500, 2000), mtry = 3:7)
 rf_tune <- tune(randomForest, No.show ~ ., data =
                    train_set, ranges = rf_ranges)
```
 
```{r rF_parameters, echo = TRUE}
 rf_tune$best.parameters
 rf_best <- rf_tune$best.model
rf_best
```

Making the prediction:

```{r rF_predict, echo =TRUE}
 ##predict
 y_hat_rf <- predict(rf_best, test_set)
```
 
Evaluating the accuracy of the randomForest model:

```{r randomForest-accuracy, echo = TRUE}
 confusionMatrix( y_hat_rf, test_set$No.show)$overall[["Accuracy"]]
```  
 
 Estimating the variable importance:
 
```{r variable-importance, echo = TRUE}
 imp <- importance(rf_best) 
 imp %>% knitr::kable()
``` 


The model is 79.6% accurate which is less than the logistic regression model(79.7%) and better than the kNN model(79%).On the basis of the variable importance estimate  it is evident tthat the variable that is most influential in the prediction of noshows in this model is the Age followed by SMS_received and Gender.We will now try to see if the ensemble method combining kNN and randomForest will provide us with an improved accuracy.
 

### Model 4: ensembles (random forest and kNN)

```{r load packages4, echo = FALSE} 
  library(caret)
``` 

Prediction with ensemble method:

```{r ensemble-predict, echo = TRUE}
p_rf <- predict(rf_best, test_set, type = "prob")
 p_rf<- p_rf / rowSums(p_rf)
 p_knn <- predict(train_knn, test_set, type = "prob")
 p <- (p_rf + p_knn)/2
 y_pred <- factor(apply(p, 1, which.max)-1)
```
 
Evaluating the accuracy:

```{r ensemble-accuracy, echo = TRUE}
 confusionMatrix(y_pred, test_set$No.show)$overall[["Accuracy"]]
```

The ensemble model improves the accuracy to 79.9% .
 
# Conclusion 
The ensemble method, combining kNN and randomForest results in an accuracy of 79.9% which is the best accuracy so far in the methods tried.The logistic regression model and the randomForest had an accuracy of 79.7% while the kNN model had an accuracy of 79%.  
 
We choose the ensemble method as our method for making the predition for the noshows. However, we also note that the  models could also have included other variables like Neighbourhood, and the period between appointment date and the scheduling date to improve upon it. This could be explored in future to see how the model improves on the accuracy.Additionally, the number of randomForest models that could be tried are limited by the time it takes to complete the calculations using the computer that I have at my disposal. 
\pagebreak

# Appendix- Environment
 
```{r, echo=FALSE}
 print("Operating System:")
 version
```

